{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "from sklearn.utils import check_array, safe_indexing, sparsefuncs_fast, check_X_y, check_random_state\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from scipy import sparse\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.base import clone\n",
    "from numbers import Integral\n",
    "from sklearn.svm import SVC\n",
    "from collections import Counter\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, f1_score, roc_auc_score\n",
    "import os\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE, SMOTENC, SVMSMOTE\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set folder paths for outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('..')\n",
    "REPO_ROOT = Path(os.getcwd())\n",
    "DATA = Path(REPO_ROOT / 'data')\n",
    "MODELS = Path(REPO_ROOT / 'model')\n",
    "REPORTS = Path(REPO_ROOT / 'reports' / 'bank')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv(DATA / 'bank_additional_full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outlier(df: pd.DataFrame, column: str, outlier_assumption: float) -> pd.DataFrame:\n",
    "    mean = np.mean(df[column])\n",
    "    std = np.std(df[column])\n",
    "    \n",
    "    minimum = mean - outlier_assumption * std\n",
    "    maximum = mean + outlier_assumption * std\n",
    "    \n",
    "    is_outlier = (df[column] < minimum) | (df[column] > maximum)\n",
    "\n",
    "    df = df[~is_outlier]\n",
    "\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= remove_outlier(df, 'age', 2.5)\n",
    "df= remove_outlier(df, 'campaign', 2.5)\n",
    "df= remove_outlier(df, 'emp.var.rate', 2.5)\n",
    "df= remove_outlier(df, 'cons.price.idx', 2.5)\n",
    "df= remove_outlier(df, 'cons.conf.idx', 2.5)\n",
    "df= remove_outlier(df, 'euribor3m', 2.5)\n",
    "df= remove_outlier(df, 'nr.employed', 2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'job', 'marital', 'education', 'default', 'housing', 'loan',\n",
       "       'contact', 'month', 'day_of_week', 'duration', 'campaign', 'pdays',\n",
       "       'previous', 'poutcome', 'emp.var.rate', 'cons.price.idx',\n",
       "       'cons.conf.idx', 'euribor3m', 'nr.employed', 'y'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_pdays_999 = df['pdays'] == 999\n",
    "df.loc[is_pdays_999, 'pdays_c'] = \"never contacted\"\n",
    "df.loc[~is_pdays_999, 'pdays_c'] = pd.qcut(df.loc[~is_pdays_999, 'pdays'], 4, labels=[\"very recently contacted\",\"recently contacted\", \"moderately recently contacted\", \"contacted long ago\"])\n",
    "df[['pdays_c', 'pdays']]\n",
    "\n",
    "df.drop('pdays', 1, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_previous_0 = df['previous'] == 0\n",
    "df.loc[~is_previous_0, 'previous_c'] = pd.cut(df.previous,bins=[0, 1, 4, 7],labels=[\"contacted once\", \"rarely contacted\", \"frequently contacted\"])\n",
    "df.loc[is_previous_0, 'previous_c'] = \"never contacted\"\n",
    "df[['previous_c', 'previous']]\n",
    "\n",
    "df.drop('previous', 1, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### In order to feed the data to any machine learning method, \n",
    "### it's convenient to change strings to numeric values. So, we are going to change 'no' to 0 and 'yes' to 1\n",
    "is_purchased = df['y'] == 'yes'\n",
    "df.loc[is_purchased, 'target'] = 1\n",
    "df.loc[~is_purchased, 'target'] = 0\n",
    "df[['target', 'y']]\n",
    "df.drop('y', 1, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>emp.var.rate</th>\n",
       "      <th>cons.price.idx</th>\n",
       "      <th>cons.conf.idx</th>\n",
       "      <th>euribor3m</th>\n",
       "      <th>nr.employed</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.006561</td>\n",
       "      <td>0.006875</td>\n",
       "      <td>0.081826</td>\n",
       "      <td>0.054775</td>\n",
       "      <td>0.109962</td>\n",
       "      <td>0.092023</td>\n",
       "      <td>0.077782</td>\n",
       "      <td>-0.024210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>duration</th>\n",
       "      <td>-0.006561</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.037328</td>\n",
       "      <td>-0.014408</td>\n",
       "      <td>0.005740</td>\n",
       "      <td>-0.012044</td>\n",
       "      <td>-0.017155</td>\n",
       "      <td>-0.024651</td>\n",
       "      <td>0.423297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>campaign</th>\n",
       "      <td>0.006875</td>\n",
       "      <td>-0.037328</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.101920</td>\n",
       "      <td>0.087973</td>\n",
       "      <td>-0.007239</td>\n",
       "      <td>0.081965</td>\n",
       "      <td>0.095540</td>\n",
       "      <td>-0.046072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emp.var.rate</th>\n",
       "      <td>0.081826</td>\n",
       "      <td>-0.014408</td>\n",
       "      <td>0.101920</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.845136</td>\n",
       "      <td>0.328424</td>\n",
       "      <td>0.978759</td>\n",
       "      <td>0.954690</td>\n",
       "      <td>-0.257349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cons.price.idx</th>\n",
       "      <td>0.054775</td>\n",
       "      <td>0.005740</td>\n",
       "      <td>0.087973</td>\n",
       "      <td>0.845136</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.190927</td>\n",
       "      <td>0.805977</td>\n",
       "      <td>0.735563</td>\n",
       "      <td>-0.185322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cons.conf.idx</th>\n",
       "      <td>0.109962</td>\n",
       "      <td>-0.012044</td>\n",
       "      <td>-0.007239</td>\n",
       "      <td>0.328424</td>\n",
       "      <td>0.190927</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.397912</td>\n",
       "      <td>0.217505</td>\n",
       "      <td>0.027898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>euribor3m</th>\n",
       "      <td>0.092023</td>\n",
       "      <td>-0.017155</td>\n",
       "      <td>0.081965</td>\n",
       "      <td>0.978759</td>\n",
       "      <td>0.805977</td>\n",
       "      <td>0.397912</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.962600</td>\n",
       "      <td>-0.248442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nr.employed</th>\n",
       "      <td>0.077782</td>\n",
       "      <td>-0.024651</td>\n",
       "      <td>0.095540</td>\n",
       "      <td>0.954690</td>\n",
       "      <td>0.735563</td>\n",
       "      <td>0.217505</td>\n",
       "      <td>0.962600</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.274426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <td>-0.024210</td>\n",
       "      <td>0.423297</td>\n",
       "      <td>-0.046072</td>\n",
       "      <td>-0.257349</td>\n",
       "      <td>-0.185322</td>\n",
       "      <td>0.027898</td>\n",
       "      <td>-0.248442</td>\n",
       "      <td>-0.274426</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     age  duration  campaign  emp.var.rate  cons.price.idx  \\\n",
       "age             1.000000 -0.006561  0.006875      0.081826        0.054775   \n",
       "duration       -0.006561  1.000000 -0.037328     -0.014408        0.005740   \n",
       "campaign        0.006875 -0.037328  1.000000      0.101920        0.087973   \n",
       "emp.var.rate    0.081826 -0.014408  0.101920      1.000000        0.845136   \n",
       "cons.price.idx  0.054775  0.005740  0.087973      0.845136        1.000000   \n",
       "cons.conf.idx   0.109962 -0.012044 -0.007239      0.328424        0.190927   \n",
       "euribor3m       0.092023 -0.017155  0.081965      0.978759        0.805977   \n",
       "nr.employed     0.077782 -0.024651  0.095540      0.954690        0.735563   \n",
       "target         -0.024210  0.423297 -0.046072     -0.257349       -0.185322   \n",
       "\n",
       "                cons.conf.idx  euribor3m  nr.employed    target  \n",
       "age                  0.109962   0.092023     0.077782 -0.024210  \n",
       "duration            -0.012044  -0.017155    -0.024651  0.423297  \n",
       "campaign            -0.007239   0.081965     0.095540 -0.046072  \n",
       "emp.var.rate         0.328424   0.978759     0.954690 -0.257349  \n",
       "cons.price.idx       0.190927   0.805977     0.735563 -0.185322  \n",
       "cons.conf.idx        1.000000   0.397912     0.217505  0.027898  \n",
       "euribor3m            0.397912   1.000000     0.962600 -0.248442  \n",
       "nr.employed          0.217505   0.962600     1.000000 -0.274426  \n",
       "target               0.027898  -0.248442    -0.274426  1.000000  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_num= df[['age','duration','campaign','emp.var.rate','cons.price.idx','cons.conf.idx','euribor3m','nr.employed','target']]\n",
    "corr_num = df_num.corr()\n",
    "corr_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emp.var.rate and nr.employed are highly correlated with euribor3m. \n",
    "# That's why former two are removed to get rid of multi-collinearity.\n",
    "# Duration column is substantially correlated to target \n",
    "# and could be a good predictor of target outcome. \n",
    "# However, one can not know call duration before making the call. \n",
    "# That's why this column is removed so that the model can generalise on unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('duration',1,inplace=True)\n",
    "df.drop('emp.var.rate',1,inplace=True)\n",
    "df.drop('nr.employed',1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature= df.drop('target',1)\n",
    "df_target= df[['target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0.0: 32599, 1.0: 3519})\n"
     ]
    }
   ],
   "source": [
    "print(Counter(df['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Num of minority instances is 3519 and majority instances is 32599"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import pickle\n",
    "from imblearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encode categorical variables\n",
    "\n",
    "class MultiColumnLabelEncoder:\n",
    "    def __init__(self,columns = None):\n",
    "        self.columns = columns # array of column names to encode\n",
    "\n",
    "    def fit(self,X,y=None):\n",
    "        return self \n",
    "\n",
    "    def transform(self,X):\n",
    "        '''\n",
    "        Transforms columns of X specified in self.columns using\n",
    "        LabelEncoder(). If no columns specified, transforms all\n",
    "        columns in X.\n",
    "        '''\n",
    "        output = X.copy()\n",
    "        if self.columns is not None:\n",
    "            for col in self.columns:\n",
    "                output[col] = LabelEncoder().fit_transform(output[col])\n",
    "        else:\n",
    "            for colname,col in output.iteritems():\n",
    "                output[colname] = LabelEncoder().fit_transform(col)\n",
    "        return output\n",
    "\n",
    "    def fit_transform(self,X,y=None):\n",
    "        return self.fit(X,y).transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split the data to be 5-fold cross-validated\n",
    "kfold = StratifiedKFold(n_splits=5, random_state=42, shuffle=False)\n",
    "#randomforest model - hyperparameter tuning using grid search\n",
    "param_grid = {\n",
    "'max_depth': [10], 'max_features': [5, 10],\n",
    "'min_samples_leaf': [3, 5], 'min_samples_split': [2, 4], 'n_estimators': [500]\n",
    "}# Create a base model\n",
    "param_grid = {'randomforestclassifier__' + key: param_grid[key] for key in param_grid}\n",
    "\n",
    "## evaluate function generates the classification report of the loaded model based on the list of x and y values\n",
    "\n",
    "def evaluate(x, y, threshold):\n",
    "    x = np.array(x)\n",
    "    y = np.ravel(y)\n",
    "    pred = (loaded_model.predict_proba(x)[:,1] >= threshold).astype(bool)\n",
    "    print(pd.crosstab(y, pred, rownames=['Actual'], colnames=['Predicted']))\n",
    "    print(classification_report(y, pred,digits=4))\n",
    "    return None;\n",
    "\n",
    "## generate_curves function creates ROC-AUC and PR-AUC curve of the loaded model and compare that wth the random classifier\n",
    "def generate_curves(filename_old, filename_new, X_test, y_test):   \n",
    "\n",
    "    f = plt.figure(figsize=(10,4))\n",
    "    ax1 = f.add_subplot(121)\n",
    "    loaded_model_old = pickle.load(open(filename_old, 'rb'))\n",
    "    loaded_model_new = pickle.load(open(filename_new, 'rb'))\n",
    "\n",
    "    test_prob_old = loaded_model_old.predict_proba(X_test)[:, 1]\n",
    "    test_prob_new = loaded_model_new.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    fpr_old, tpr_old, _ = roc_curve(y_test,  test_prob_old)\n",
    "    roc_auc_old = roc_auc_score(y_test,  test_prob_old)\n",
    "    ax1.plot([0, 1], [0, 1], linestyle='--',label ='random, auc = %.3f'% 0.5, c = 'blue')\n",
    "    ax1.plot(fpr_old, tpr_old ,label ='SMOTE-NC, auc = %.3f'% roc_auc_old, c= 'green')\n",
    "    \n",
    "    fpr_new, tpr_new, _ = roc_curve(y_test,  test_prob_new)\n",
    "    roc_auc_new = roc_auc_score(y_test,  test_prob_new)\n",
    "    ax1.plot(fpr_new, tpr_new ,label ='SMOTE-ENC, auc = %.3f'% roc_auc_new, c= 'red')\n",
    "    \n",
    "    ax1.legend(loc=4)\n",
    "\n",
    "    ax1.set_title('ROC curve' ,fontsize=16)\n",
    "    ax1.set_ylabel('True Positive Rate',fontsize=14)\n",
    "    ax1.set_xlabel('False Positive Rate',fontsize=14)\n",
    "\n",
    "    ax2 = f.add_subplot(122)\n",
    "    \n",
    "\n",
    "    precision_old, recall_old, _ = precision_recall_curve(y_test, test_prob_old)\n",
    "    precision_new, recall_new, _ = precision_recall_curve(y_test, test_prob_new)\n",
    "    \n",
    "    auc_score_old = auc(recall_old, precision_old)\n",
    "    auc_score_new = auc(recall_new, precision_new)\n",
    "    \n",
    "    random_auc = y_test.sum()/len(y_test)\n",
    "    \n",
    "    ax2.plot([0, 1], [random_auc, random_auc], linestyle='--', label ='random, auc = %.3f'% random_auc, c ='blue')\n",
    "    ax2.plot(recall_old, precision_old, label = 'SMOTE-NC, auc=%.3f'% auc_score_old, c = 'green')\n",
    "    ax2.plot(recall_new, precision_new, label = 'SMOTE-ENC, auc=%.3f'% auc_score_new, c = 'red')\n",
    "    \n",
    "    ax2.set_title('Precision Recall curve' ,fontsize=16)\n",
    "    ax2.set_ylabel('Precision', fontsize=14)\n",
    "    ax2.set_xlabel('Recall',fontsize=14)\n",
    "    ax2.legend(loc='best')\n",
    "    plt.show()\n",
    "    f.savefig(REPORTS / 'bank_roc_prc.jpeg', bbox_inches='tight')\n",
    "    \n",
    "    return None;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_df = MultiColumnLabelEncoder(columns = [ 'job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'day_of_week',\n",
    "                                               'poutcome', 'pdays_c', 'previous_c']).fit_transform(df_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'job', 'marital', 'education', 'default', 'housing', 'loan',\n",
       "       'contact', 'month', 'day_of_week', 'campaign', 'poutcome',\n",
       "       'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'pdays_c',\n",
       "       'previous_c'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the raw data into train and test set. Split ratio = 75:25\n",
    "X_train, X_test, y_train, y_test = train_test_split(encoded_df, df_target, test_size=0.25, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list = list(X_train.columns)\n",
    "X_train.index = pd.RangeIndex(len(X_train.index))\n",
    "y_train.index = pd.RangeIndex(len(y_train.index))\n",
    "X_test.index = pd.RangeIndex(len(X_test.index))\n",
    "y_test.index = pd.RangeIndex(len(y_test.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply standard scaler on the features , so that euclidean distance calculation in SMOTE is not biased\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(X_train, index=range(X_train.shape[0]),\n",
    "                          columns=col_list)\n",
    "X_test = pd.DataFrame(X_test, index=range(X_test.shape[0]),\n",
    "                          columns=col_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our New Proposed SMOTE Method\n",
    "from scipy import stats\n",
    "class MySMOTENC():\n",
    "    \n",
    "    def __init__(self, categorical_features):\n",
    "        self.categorical_features = categorical_features\n",
    "        \n",
    "    def chk_neighbors(self, nn_object, additional_neighbor):\n",
    "        if isinstance(nn_object, Integral):\n",
    "            return NearestNeighbors(n_neighbors=nn_object + additional_neighbor)\n",
    "        elif isinstance(nn_object, KNeighborsMixin):\n",
    "            return clone(nn_object)\n",
    "        else:\n",
    "            raise_isinstance_error(nn_name, [int, KNeighborsMixin], nn_object)     \n",
    "    \n",
    "    def generate_samples(self, X, nn_data, nn_num, rows, cols, steps, continuous_features_,):\n",
    "        rng = check_random_state(42)\n",
    "\n",
    "        diffs = nn_data[nn_num[rows, cols]] - X[rows]\n",
    "\n",
    "        if sparse.issparse(X):\n",
    "            sparse_func = type(X).__name__\n",
    "            steps = getattr(sparse, sparse_func)(steps)\n",
    "            X_new = X[rows] + steps.multiply(diffs)\n",
    "        else:\n",
    "            X_new = X[rows] + steps * diffs \n",
    "\n",
    "        X_new = (X_new.tolil() if sparse.issparse(X_new) else X_new)\n",
    "        # convert to dense array since scipy.sparse doesn't handle 3D\n",
    "        nn_data = (nn_data.toarray() if sparse.issparse(nn_data) else nn_data)\n",
    "\n",
    "        all_neighbors = nn_data[nn_num[rows]]\n",
    "\n",
    "        for idx in range(continuous_features_.size, X.shape[1]):\n",
    "\n",
    "            mode = stats.mode(all_neighbors[:, :, idx], axis = 1)[0]\n",
    "\n",
    "            X_new[:, idx] = np.ravel(mode)            \n",
    "        return X_new\n",
    "    \n",
    "    def make_samples(self, X, y_dtype, y_type, nn_data, nn_num, n_samples, continuous_features_, step_size=1.0):\n",
    "        random_state = check_random_state(42)\n",
    "        samples_indices = random_state.randint(low=0, high=len(nn_num.flatten()), size=n_samples)    \n",
    "        steps = step_size * random_state.uniform(size=n_samples)[:, np.newaxis]\n",
    "        rows = np.floor_divide(samples_indices, nn_num.shape[1])\n",
    "        cols = np.mod(samples_indices, nn_num.shape[1])\n",
    "\n",
    "        X_new = self.generate_samples(X, nn_data, nn_num, rows, cols, steps, continuous_features_)\n",
    "        y_new = np.full(n_samples, fill_value=y_type, dtype=y_dtype)\n",
    "        \n",
    "        return X_new, y_new\n",
    "    \n",
    "    def cat_corr_pandas(self, X, target_df, target_column, target_value):\n",
    "    # X has categorical columns\n",
    "        categorical_columns = list(X.columns)\n",
    "        X = pd.concat([X, target_df], axis=1)\n",
    "\n",
    "        # filter X for target value\n",
    "        is_target = X.loc[:, target_column] == target_value\n",
    "        X_filtered = X.loc[is_target, :]\n",
    "\n",
    "        X_filtered.drop(target_column, axis=1, inplace=True)\n",
    "\n",
    "        # get columns in X\n",
    "        nrows = len(X)\n",
    "        encoded_dict_list = []\n",
    "        nan_dict = dict({})\n",
    "        c = 0\n",
    "        imb_ratio = len(X_filtered)/len(X)\n",
    "        OE_dict = {}\n",
    "        \n",
    "        for column in categorical_columns:\n",
    "            for level in list(X.loc[:, column].unique()):\n",
    "                \n",
    "                # filter rows where level is present\n",
    "                row_level_filter = X.loc[:, column] == level\n",
    "                rows_in_level = len(X.loc[row_level_filter, :])\n",
    "                \n",
    "                # number of rows in level where target is 1\n",
    "                O = len(X.loc[is_target & row_level_filter, :])\n",
    "                E = rows_in_level * imb_ratio\n",
    "                # Encoded value = chi, i.e. (observed - expected)/expected\n",
    "                ENC = (O - E) / E\n",
    "                OE_dict[level] = ENC\n",
    "                \n",
    "            encoded_dict_list.append(OE_dict)\n",
    "\n",
    "            X.loc[:, column] = X[column].map(OE_dict)\n",
    "            nan_idx_array = np.ravel(np.argwhere(np.isnan(X.loc[:, column])))\n",
    "            if len(nan_idx_array) > 0 :\n",
    "                nan_dict[c] = nan_idx_array\n",
    "            c = c + 1\n",
    "            X.loc[:, column].fillna(-1, inplace = True)\n",
    "            \n",
    "        X.drop(target_column, axis=1, inplace=True)\n",
    "        return X, encoded_dict_list, nan_dict\n",
    "\n",
    "    def fit_resample(self, X, y):\n",
    "        X_cat_encoded, encoded_dict_list, nan_dict = self.cat_corr_pandas(X.iloc[:,np.asarray(self.categorical_features)], y, target_column='target', target_value=1)\n",
    "#         X_cat_encoded = np.ravel(np.array(X_cat_encoded))\n",
    "        X_cat_encoded = np.array(X_cat_encoded)\n",
    "        y = np.ravel(y)\n",
    "        X = np.array(X)\n",
    "\n",
    "        unique, counts = np.unique(y, return_counts=True)\n",
    "        target_stats = dict(zip(unique, counts))\n",
    "        n_sample_majority = max(target_stats.values())\n",
    "        class_majority = max(target_stats, key=target_stats.get)\n",
    "        sampling_strategy = {key: n_sample_majority - value for (key, value) in target_stats.items() if key != class_majority}\n",
    "\n",
    "        n_features_ = X.shape[1]\n",
    "        categorical_features = np.asarray(self.categorical_features)\n",
    "        if categorical_features.dtype.name == 'bool':\n",
    "            categorical_features_ = np.flatnonzero(categorical_features)\n",
    "        else:\n",
    "            if any([cat not in np.arange(n_features_) for cat in categorical_features]):\n",
    "                raise ValueError('Some of the categorical indices are out of range. Indices'\n",
    "                            ' should be between 0 and {}'.format(n_features_))\n",
    "            categorical_features_ = categorical_features\n",
    "\n",
    "        continuous_features_ = np.setdiff1d(np.arange(n_features_),categorical_features_)\n",
    "\n",
    "        target_stats = Counter(y)\n",
    "        class_minority = min(target_stats, key=target_stats.get)\n",
    "\n",
    "        X_continuous = X[:, continuous_features_]\n",
    "        X_continuous = check_array(X_continuous, accept_sparse=['csr', 'csc'])\n",
    "        X_minority = safe_indexing(X_continuous, np.flatnonzero(y == class_minority))\n",
    "\n",
    "        if sparse.issparse(X):\n",
    "            if X.format == 'csr':\n",
    "                _, var = sparsefuncs_fast.csr_mean_variance_axis0(X_minority)\n",
    "            else:\n",
    "                _, var = sparsefuncs_fast.csc_mean_variance_axis0(X_minority)\n",
    "        else:\n",
    "            var = X_minority.var(axis=0)\n",
    "        median_std_ = np.median(np.sqrt(var))\n",
    "\n",
    "        X_categorical = X[:, categorical_features_]\n",
    "        X_copy = np.hstack((X_continuous, X_categorical))\n",
    "\n",
    "        X_cat_encoded = X_cat_encoded * median_std_\n",
    "\n",
    "        X_encoded = np.hstack((X_continuous, X_cat_encoded))\n",
    "        X_resampled = X_encoded.copy()\n",
    "        y_resampled = y.copy()\n",
    "\n",
    "\n",
    "        for class_sample, n_samples in sampling_strategy.items():\n",
    "            if n_samples == 0:\n",
    "                continue\n",
    "            target_class_indices = np.flatnonzero(y == class_sample)\n",
    "            X_class = safe_indexing(X_encoded, target_class_indices)\n",
    "            nn_k_ = self.chk_neighbors(5, 1)\n",
    "            nn_k_.fit(X_class)\n",
    "\n",
    "            nns = nn_k_.kneighbors(X_class, return_distance=False)[:, 1:]\n",
    "            X_new, y_new = self.make_samples(X_class, y.dtype, class_sample, X_class, nns, n_samples, continuous_features_, 1.0)\n",
    "\n",
    "            if sparse.issparse(X_new):\n",
    "                X_resampled = sparse.vstack([X_resampled, X_new])\n",
    "                sparse_func = 'tocsc' if X.format == 'csc' else 'tocsr'\n",
    "                X_resampled = getattr(X_resampled, sparse_func)()\n",
    "            else:\n",
    "                X_resampled = np.vstack((X_resampled, X_new))\n",
    "            y_resampled = np.hstack((y_resampled, y_new))\n",
    "            \n",
    "        X_resampled_copy = X_resampled.copy()\n",
    "        i = 0\n",
    "        for col in range(continuous_features_.size, X.shape[1]):\n",
    "            encoded_dict = encoded_dict_list[i]\n",
    "            i = i + 1\n",
    "            for key, value in encoded_dict.items():\n",
    "                X_resampled_copy[:, col] = np.where(np.round(X_resampled_copy[:, col], 4) == np.round(value * median_std_, 4), key, X_resampled_copy[:, col])\n",
    "\n",
    "        for key, value in nan_dict.items():\n",
    "            for item in value:\n",
    "                X_resampled_copy[item, continuous_features_.size + key] = X_copy[item, continuous_features_.size + key]\n",
    "\n",
    "               \n",
    "        X_resampled = X_resampled_copy   \n",
    "        indices_reordered = np.argsort(np.hstack((continuous_features_, categorical_features_)))\n",
    "        if sparse.issparse(X_resampled):\n",
    "            col_indices = X_resampled.indices.copy()\n",
    "            for idx, col_idx in enumerate(indices_reordered):\n",
    "                mask = X_resampled.indices == col_idx\n",
    "                col_indices[mask] = idx\n",
    "            X_resampled.indices = col_indices\n",
    "        else:\n",
    "            X_resampled = X_resampled[:, indices_reordered]\n",
    "        return X_resampled, y_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply the random-forest classifier along with the NEW SMOTE method\n",
    "filename = (MODELS / 'bank_mysmotenc.sav')\n",
    "samp_pipeline = make_pipeline(MySMOTENC(categorical_features = [1,2,3,4,5,6,7,8,9,11,15,16]), \n",
    "                              RandomForestClassifier(random_state=42))\n",
    "# check model performance on different values of hyper-parameters.\n",
    "grid_search = GridSearchCV(samp_pipeline, param_grid=param_grid, cv=kfold, scoring='balanced_accuracy',\n",
    "                        return_train_score=True, n_jobs = -1, verbose = 2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_grid = grid_search.best_estimator_\n",
    "pickle.dump(best_grid, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply the random-forest classifier along with the existng SMOTE-NC method\n",
    "filename = (MODELS / 'bank_smotenc.sav')\n",
    "samp_pipeline = make_pipeline(SMOTENC(random_state=42, categorical_features = [1,2,3,4,5,6,7,8,9,11,15,16]), \n",
    "                              RandomForestClassifier(random_state=42))\n",
    "# check model performance on different values of hyper-parameters.\n",
    "grid_search = GridSearchCV(samp_pipeline, param_grid=param_grid, cv=kfold, scoring='balanced_accuracy',\n",
    "                        return_train_score=True, n_jobs = -1, verbose = 2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_grid = grid_search.best_estimator_\n",
    "pickle.dump(best_grid, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluate SMOTE-NC performance on train and test set\n",
    "\n",
    "filename = (MODELS / 'bank_smotenc.sav')\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "threshold = 0.5\n",
    "print('                RF performance Report')\n",
    "\n",
    "evaluate(X_train, y_train, threshold)\n",
    "evaluate(X_test, y_test, threshold)\n",
    "loaded_model.steps[1][1].feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluate SMOTE-ENC performance on train and test set\n",
    "\n",
    "filename = (MODELS / 'bank_mysmotenc.sav')\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "threshold = 0.5\n",
    "print('                RF performance Report')\n",
    "evaluate(X_train, y_train, threshold)\n",
    "evaluate(X_test, y_test, threshold)\n",
    "loaded_model.steps[1][1].feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare variable importance between SMOTE-NC and SMOTE-ENC\n",
    "filename = (MODELS / 'bank_mysmotenc.sav')\n",
    "loaded_model1 = pickle.load(open(filename, 'rb'))\n",
    "filename = (MODELS / 'bank_smotenc.sav')\n",
    "loaded_model2 = pickle.load(open(filename, 'rb'))\n",
    "feature_df = pd.DataFrame(np.vstack((loaded_model1.steps[1][1].feature_importances_,loaded_model2.steps[1][1].feature_importances_,\n",
    "                         (loaded_model1.steps[1][1].feature_importances_ - loaded_model2.steps[1][1].feature_importances_))))\n",
    "feature_df.index = ('new', 'old', 'imp_increase')\n",
    "feature_df.columns = X_train.columns\n",
    "feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Create ROC-AUC and PR-AUC curve\n",
    "generate_curves(MODELS / 'bank_smotenc.sav', MODELS / 'bank_mysmotenc.sav', X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot feature importance using our new SMOTE-ENC method\n",
    "\n",
    "filename = (MODELS / 'bank_mysmotenc.sav')\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "var_imp = (pd.Series(loaded_model.steps[1][1].feature_importances_, index=X_train.columns).nlargest(20))\n",
    "var_imp_df = var_imp.reset_index()\n",
    "var_imp_df.columns = ['Feature', 'Importance using SMOTE-ENC']\n",
    "var_imp_df.set_index('Feature', inplace=True)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "rcParams.update({'figure.autolayout': True})\n",
    "var_imp_df.plot(kind='barh').invert_yaxis()\n",
    "plt.savefig(REPORTS / 'bank_newsmote.jpeg', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot feature importance using the existing SMOTE-NC method\n",
    "\n",
    "filename = (MODELS / 'bank_smotenc.sav')\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "var_imp = (pd.Series(loaded_model.steps[1][1].feature_importances_, index=X_train.columns).nlargest(20))\n",
    "var_imp_df = var_imp.reset_index()\n",
    "var_imp_df.columns = ['Feature', 'Importance using SMOTE-NC']\n",
    "var_imp_df.set_index('Feature', inplace=True)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "rcParams.update({'figure.autolayout': True})\n",
    "var_imp_df.plot(kind='barh').invert_yaxis()\n",
    "plt.savefig(REPORTS / 'bank_oldsmote.jpeg', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "filename = (MODELS / 'bank_mysmotenc.sav')\n",
    "model1 = pickle.load(open(filename, 'rb'))\n",
    "cv_scoremysmote = cross_val_score(model1, X_train, y_train, cv=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = (MODELS / 'bank_smotenc.sav')\n",
    "model2 = pickle.load(open(filename, 'rb'))\n",
    "cv_scoresmote = cross_val_score(model2, X_train, y_train, cv=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=5.468121277760906, pvalue=0.0010064715466962366)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.ttest_ind(cv_scoremysmote,cv_scoresmote, equal_var = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.8145072 , 0.82668882, 0.82595054, 0.81761122, 0.82665682])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_scoresmote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.85252861, 0.84348468, 0.83573274, 0.85102455, 0.85896253])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_scoremysmote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (SMOTE_ENC)",
   "language": "python",
   "name": "smote_enc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
